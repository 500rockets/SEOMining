#!/usr/bin/env python
"""
GPU-Intensive Content Generation using REAL downloaded competitor data
Uses the actual 10 competitor pages we already scraped
"""
import sys
sys.path.insert(0, '/app')

import asyncio
import json
import time
from datetime import datetime
from app.services.optimization import get_content_generator
from app.services.scraping import get_scraping_service


async def run_real_gpu_analysis():
    """
    Run GPU-intensive analysis using the actual downloaded competitor data
    """
    print("ðŸ”¥ GPU-INTENSIVE ANALYSIS WITH REAL DATA")
    print("=" * 80)
    print("Using actual downloaded competitor pages from previous analysis")
    print("=" * 80)
    print()
    
    # Load the real analysis data
    print("[1/6] Loading real competitor data...")
    try:
        with open('/app/app/500rockets_analysis_20251015_182119.json', 'r', encoding='utf-16') as f:
            analysis_data = json.load(f)
        
        competitor_urls = [comp['url'] for comp in analysis_data['competitors']]
        target_url = analysis_data['target_url']
        query = analysis_data['query']
        
        print(f"âœ“ Target: {target_url}")
        print(f"âœ“ Query: '{query}'")
        print(f"âœ“ Competitors: {len(competitor_urls)}")
        
        # Show competitor details
        print("\nCompetitor Details:")
        for i, comp in enumerate(analysis_data['competitors'][:5], 1):
            print(f"  {i}. {comp['url']} (Score: {comp['composite_score']:.1f})")
        
    except Exception as e:
        print(f"âœ— Could not load analysis data: {e}")
        return
    
    print()
    
    # Initialize services
    print("[2/6] Initializing GPU-accelerated services...")
    generator = await get_content_generator()
    scraper = get_scraping_service()
    print("âœ“ Services ready (2x RTX 4000 GPUs active)")
    print()
    
    # Re-scrape content for fresh analysis
    print("[3/6] Re-scraping competitor content for intensive analysis...")
    print("  (This will take a few minutes to get fresh content)")
    
    competitor_contents = []
    for i, url in enumerate(competitor_urls, 1):
        try:
            print(f"  Scraping {i}/{len(competitor_urls)}: {url[:60]}...")
            content = await scraper.scrape_url(url, use_proxy=True)
            if content and content.get('content'):
                competitor_contents.append({
                    'url': url,
                    'content': content['content'],
                    'title': content.get('title', ''),
                    'meta_description': content.get('meta_description', '')
                })
                print(f"    âœ“ {len(content['content'])} characters")
            else:
                print(f"    âœ— No content")
        except Exception as e:
            print(f"    âœ— Error: {str(e)[:50]}...")
            continue
    
    print(f"âœ“ Successfully scraped {len(competitor_contents)}/{len(competitor_urls)} competitors")
    print()
    
    # Scrape target content
    print("[4/6] Scraping target content...")
    try:
        target_content = await scraper.scrape_url(target_url, use_proxy=True)
        if not target_content or not target_content.get('content'):
            print("âœ— Could not scrape target content")
            return
        
        print(f"âœ“ Target content: {len(target_content['content'])} characters")
    except Exception as e:
        print(f"âœ— Error scraping target: {str(e)}")
        return
    
    print()
    
    # Run intensive GPU analysis
    print("[5/6] Running GPU-intensive semantic analysis...")
    print("  ðŸ”¥ This will use both GPUs intensively!")
    print("  ðŸ“Š Processing thousands of phrases from real competitor content")
    print("  âš¡ GPU acceleration: ~20x faster than CPU")
    print()
    
    start_time = time.time()
    
    result = await generator.generate_optimized_content(
        target_url=target_url,
        query=query,
        competitor_urls=[comp['url'] for comp in competitor_contents],
        run_duration_minutes=10  # 10-minute intensive analysis
    )
    
    end_time = time.time()
    duration = (end_time - start_time) / 60
    
    print(f"âœ“ GPU analysis complete! Duration: {duration:.1f} minutes")
    print()
    
    # Display results
    print("[6/6] Analysis Results")
    print("=" * 80)
    print()
    
    metadata = result['generation_metadata']
    print(f"Analysis Duration: {metadata['analysis_duration_minutes']} minutes")
    print(f"Competitors Analyzed: {metadata['competitors_analyzed']}")
    print(f"Phrases Extracted: {metadata['phrases_extracted']}")
    print(f"Semantic Gaps Found: {metadata['gaps_found']}")
    print(f"Generated At: {metadata['generated_at']}")
    print()
    
    # Show top semantic gaps
    print("TOP SEMANTIC GAPS IDENTIFIED:")
    print("-" * 80)
    
    semantic_gaps = result['semantic_insights']['semantic_gaps']
    for i, gap in enumerate(semantic_gaps[:10], 1):
        print(f"{i}. \"{gap['phrase']}\"")
        print(f"   Impact: +{gap['estimated_impact']:.1f} points")
        print(f"   Query Match: {gap['query_similarity']*100:.1f}%")
        print(f"   Used by: {gap['competitor_usage']} competitors")
        print()
    
    # Save results
    print("Saving results...")
    
    # Save optimized content
    optimized_content = result['optimized_content']
    with open('/app/real_gpu_optimized_content.md', 'w', encoding='utf-8') as f:
        f.write(optimized_content)
    
    # Save full analysis
    with open('/app/real_gpu_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, default=str)
    
    print("âœ“ Optimized content: real_gpu_optimized_content.md")
    print("âœ“ Full analysis: real_gpu_analysis_results.json")
    print()
    
    # Show content preview
    print("OPTIMIZED CONTENT PREVIEW:")
    print("=" * 80)
    preview = optimized_content[:1500] + "..." if len(optimized_content) > 1500 else optimized_content
    print(preview)
    print()
    
    print("=" * 80)
    print("ðŸŽ¯ GPU-INTENSIVE ANALYSIS COMPLETE!")
    print("=" * 80)
    print()
    print("ðŸ“Š Used REAL competitor data from previous analysis")
    print("ðŸ”¥ GPU-intensive processing: COMPLETE")
    print("ðŸ“„ Ready-to-implement content generated")
    print("âš¡ Both RTX 4000 GPUs utilized for maximum optimization")
    print()
    print("Next: Review real_gpu_optimized_content.md and implement!")


if __name__ == "__main__":
    asyncio.run(run_real_gpu_analysis())

